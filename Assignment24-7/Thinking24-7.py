1、参数共享指的是什么
顾名思义就是不同位置对图像使用相同的参数，减少了参数的数量，释放了空间。

2、为什么会用到batch normalization
深度学习问题，随着网络层数对加深，每次经过一层对输入对分布会发生改变，bn的作用是为了将分布拉回到均值为0方差为1的标准正态分布，加快收敛速度使得模型可以更快达到局部最优，缓解梯度消失和梯度爆炸的问题。

3、使用dropout可以解决什么问题？
dropout归根结底是一个缓解过拟合的手段，他使得数据在神经网络的层级之间传递的时候随机关闭一些神经元，对剩下神经元对参数进行反向传播优化，使得模型对泛化能力更强，更加的健壮。

